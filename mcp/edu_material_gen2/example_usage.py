"""
æ–‡æ¡£åˆ†å—å¤„ç†å™¨ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨DocumentChunkerå¤„ç†æ–‡æ¡£å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
"""

import asyncio
import os
from pathlib import Path
from typing import List, Dict, Any
from docx import Document
from document_chunker import DocumentChunker


def read_word_document(file_path: str) -> str:
    """
    ä»Wordæ–‡æ¡£ä¸­è¯»å–æ–‡æœ¬å†…å®¹
    
    Args:
        file_path: Wordæ–‡æ¡£æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in ['.docx', '.doc']:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œä»…æ”¯æŒ .docx å’Œ .doc æ–‡ä»¶")
        
        # ä½¿ç”¨æ”¹è¿›çš„Wordæ–‡æ¡£è¯»å–æ–¹æ³•
        content = _extract_word_content_standalone(file_path)
        
        print(f"æˆåŠŸè¯»å–Wordæ–‡æ¡£: {file_path}")
        print(f"æ€»å­—ç¬¦æ•°: {len(content)}")
        
        return content
        
    except Exception as e:
        print(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {e}")
        raise


def _extract_word_content_standalone(file_path: str) -> str:
    """
    ç‹¬ç«‹çš„Wordæ–‡æ¡£å†…å®¹æå–å‡½æ•°
    
    Args:
        file_path: Wordæ–‡æ¡£æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„å®Œæ•´æ–‡æœ¬å†…å®¹
    """
    from docx import Document
    from docx.oxml.table import CT_Tbl
    from docx.oxml.text.paragraph import CT_P
    from docx.table import Table
    from docx.text.paragraph import Paragraph
    
    doc = Document(file_path)
    full_text = []
    
    def extract_table_text(table):
        """æå–è¡¨æ ¼ä¸­çš„æ–‡æœ¬"""
        table_text = []
        for row in table.rows:
            row_text = []
            for cell in row.cells:
                cell_text = []
                for paragraph in cell.paragraphs:
                    if paragraph.text.strip():
                        cell_text.append(paragraph.text.strip())
                if cell_text:
                    row_text.append(' '.join(cell_text))
            if row_text:
                table_text.append(' | '.join(row_text))
        return '\n'.join(table_text)
    
    # æ–¹æ³•1: éå†æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…ƒç´ 
    try:
        for element in doc.element.body:
            if isinstance(element, CT_P):
                # æ®µè½
                paragraph = Paragraph(element, doc)
                text = paragraph.text.strip()
                if text:
                    full_text.append(text)
            elif isinstance(element, CT_Tbl):
                # è¡¨æ ¼
                table = Table(element, doc)
                table_content = extract_table_text(table)
                if table_content:
                    full_text.append(table_content)
    except Exception as e:
        print(f"æ–¹æ³•1æå–å¤±è´¥: {e}")
    
    # å¦‚æœæ–¹æ³•1æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
    if not full_text:
        print("ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æå–Wordæ–‡æ¡£å†…å®¹")
        
        # å¤‡ç”¨æ–¹æ³•: ç›´æ¥æå–æ‰€æœ‰æ®µè½å’Œè¡¨æ ¼
        for paragraph in doc.paragraphs:
            text = paragraph.text.strip()
            if text:
                full_text.append(text)
        
        for table in doc.tables:
            table_content = extract_table_text(table)
            if table_content:
                full_text.append(table_content)
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´æ·±å±‚çš„æå–
    if not full_text:
        print("ä½¿ç”¨æ·±åº¦æå–æ–¹æ³•")
        for paragraph in doc.paragraphs:
            paragraph_text = []
            for run in paragraph.runs:
                if run.text.strip():
                    paragraph_text.append(run.text.strip())
            if paragraph_text:
                full_text.append(' '.join(paragraph_text))
    
    result = '\n\n'.join(full_text)
    
    if not result.strip():
        print("è­¦å‘Š: æ— æ³•ä»Wordæ–‡æ¡£ä¸­æå–ä»»ä½•æ–‡æœ¬å†…å®¹")
        # å°è¯•è¯»å–æ–‡æ¡£çš„åŸºæœ¬ä¿¡æ¯
        try:
            core_props = doc.core_properties
            if hasattr(core_props, 'title') and core_props.title:
                result = f"æ–‡æ¡£æ ‡é¢˜: {core_props.title}"
                print(f"ä»…æå–åˆ°æ–‡æ¡£æ ‡é¢˜: {core_props.title}")
        except:
            pass
    
    return result


async def process_word_document(chunker: DocumentChunker, file_path: str, doc_category: str, chunk_size: int = 512):
    """
    å¤„ç†Wordæ–‡æ¡£å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
    
    Args:
        chunker: æ–‡æ¡£åˆ†å—å¤„ç†å™¨å®ä¾‹
        file_path: Wordæ–‡æ¡£æ–‡ä»¶è·¯å¾„
        doc_category: æ–‡æ¡£ç±»åˆ«
        chunk_size: åˆ†å—å¤§å°
        
    Returns:
        æ’å…¥è®°å½•çš„IDåˆ—è¡¨
    """
    try:
        print(f"å¼€å§‹å¤„ç†Wordæ–‡æ¡£: {file_path}")
        
        # è¯»å–Wordæ–‡æ¡£å†…å®¹
        text_content = read_word_document(file_path)
        
        if not text_content.strip():
            print("è­¦å‘Š: Wordæ–‡æ¡£å†…å®¹ä¸ºç©º")
            return []
        
        # ä»æ–‡ä»¶è·¯å¾„æå–æ–‡æ¡£æ ‡é¢˜
        from pathlib import Path
        doc_title = Path(file_path).stem
        
        # ä½¿ç”¨chunkerå¤„ç†æ–‡æœ¬å†…å®¹
        inserted_ids = await chunker.process_text_content(
            text_content=text_content,
            doc_category=doc_category,
            doc_title=doc_title,
            chunk_size=chunk_size
        )
        
        print(f"Wordæ–‡æ¡£å¤„ç†å®Œæˆï¼æ’å…¥äº† {len(inserted_ids)} æ¡è®°å½•")
        print(f"è®°å½•ID: {inserted_ids}")
        
        return inserted_ids
        
    except Exception as e:
        print(f"å¤„ç†Wordæ–‡æ¡£å¤±è´¥: {e}")
        raise


async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºæ–‡æ¡£åˆ†å—å¤„ç†å™¨å®ä¾‹
    chunker = DocumentChunker(region_name="us-east-1")
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        await chunker.init_db_client()
        print("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        
        # ç¤ºä¾‹1: å¤„ç†æ–‡æœ¬å†…å®¹
        sample_text = """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ
        å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
        
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ˜¯ä½¿è®¡ç®—æœºå…·æœ‰æ™ºèƒ½çš„æ ¹æœ¬é€”å¾„ã€‚æœºå™¨å­¦ä¹ çš„åº”ç”¨å·²ç»éåŠäººå·¥æ™ºèƒ½çš„å„ä¸ªåˆ†æ”¯ï¼Œ
        å¦‚ä¸“å®¶ç³»ç»Ÿã€è‡ªåŠ¨å®šç†è¯æ˜ã€è‡ªç„¶è¯­è¨€ç†è§£ã€æ¨¡å¼è¯†åˆ«ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚
        
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤šå±‚æ¬¡çš„ç¥ç»ç½‘ç»œæ¥è¿›è¡Œå­¦ä¹ å’Œè¡¨ç¤ºã€‚
        æ·±åº¦å­¦ä¹ é€šè¿‡ç»„åˆä½å±‚ç‰¹å¾å½¢æˆæ›´åŠ æŠ½è±¡çš„é«˜å±‚è¡¨ç¤ºå±æ€§ç±»åˆ«æˆ–ç‰¹å¾ï¼Œä»¥å‘ç°æ•°æ®çš„åˆ†å¸ƒå¼ç‰¹å¾è¡¨ç¤ºã€‚
        
        è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚
        æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ï¼›è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬å¤šä¸ªæ–¹é¢å’Œæ­¥éª¤ï¼ŒåŸºæœ¬æœ‰è®¤çŸ¥ã€ç†è§£ã€ç”Ÿæˆç­‰éƒ¨åˆ†ã€‚
        """
        
        print("å¼€å§‹å¤„ç†ç¤ºä¾‹æ–‡æœ¬...")
        inserted_ids = await chunker.process_text_content(
            text_content=sample_text,
            doc_category="äººå·¥æ™ºèƒ½åŸºç¡€",
            doc_title="äººå·¥æ™ºèƒ½åŸºç¡€çŸ¥è¯†",
            chunk_size=64  # è¾ƒå°çš„åˆ†å—å¤§å°ç”¨äºæ¼”ç¤º
        )
        
        print(f"æ–‡æœ¬å¤„ç†å®Œæˆï¼æ’å…¥äº† {len(inserted_ids)} æ¡è®°å½•")
        print(f"è®°å½•ID: {inserted_ids}")
        
        # ç¤ºä¾‹2: å¤„ç†Wordæ–‡æ¡£
        # è¯·å°†ä¸‹é¢çš„è·¯å¾„æ›¿æ¢ä¸ºä½ çš„å®é™…Wordæ–‡æ¡£è·¯å¾„
        word_file_path = "sample_document.docx"  # æ›¿æ¢ä¸ºå®é™…çš„Wordæ–‡æ¡£è·¯å¾„
        
        if os.path.exists(word_file_path):
            print(f"\nå¼€å§‹å¤„ç†Wordæ–‡æ¡£: {word_file_path}")
            word_ids = await process_word_document(
                chunker=chunker,
                file_path=word_file_path,
                doc_category="Wordæ–‡æ¡£",
                chunk_size=512
            )
            print(f"Wordæ–‡æ¡£å¤„ç†å®Œæˆï¼æ’å…¥äº† {len(word_ids)} æ¡è®°å½•")
        else:
            print(f"\nWordæ–‡æ¡£ä¸å­˜åœ¨: {word_file_path}")
            print("è¯·å°† word_file_path å˜é‡è®¾ç½®ä¸ºå®é™…çš„Wordæ–‡æ¡£è·¯å¾„")
        
        # ç¤ºä¾‹3: æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£
        # documents_dir = "./documents"  # æ›¿æ¢ä¸ºå®é™…çš„æ–‡æ¡£ç›®å½•è·¯å¾„
        # if os.path.exists(documents_dir):
        #     print(f"\nå¼€å§‹æ‰¹é‡å¤„ç†ç›®å½•: {documents_dir}")
        #     batch_result = await process_directory_documents(
        #         directory_path=documents_dir,
        #         doc_category="æ‰¹é‡æ–‡æ¡£",
        #         chunk_size=512,
        #         recursive=True,  # é€’å½’å¤„ç†å­ç›®å½•
        #         max_concurrent=3  # æœ€å¤§å¹¶å‘æ•°
        #     )
        #     print(f"æ‰¹é‡å¤„ç†å®Œæˆï¼")
        #     print(f"æ€»æ–‡ä»¶æ•°: {batch_result['total_files']}")
        #     print(f"æˆåŠŸå¤„ç†: {batch_result['processed_files']}")
        #     print(f"å¤„ç†å¤±è´¥: {batch_result['failed_files']}")
        #     print(f"æ€»è®°å½•æ•°: {batch_result['total_records']}")
        # else:
        #     print(f"\næ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {documents_dir}")
        
        # ç¤ºä¾‹4: å¦‚æœæœ‰æ–‡æœ¬æ–‡ä»¶ï¼Œå¯ä»¥è¿™æ ·å¤„ç†
        # sample_file = "sample_document.txt"
        # if os.path.exists(sample_file):
        #     print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {sample_file}")
        #     file_ids = await chunker.process_document_file(
        #         file_path=sample_file,
        #         doc_category="æ•™å­¦æ–‡æ¡£",
        #         chunk_size=512
        #     )
        #     print(f"æ–‡ä»¶å¤„ç†å®Œæˆï¼æ’å…¥äº† {len(file_ids)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        await chunker.close_connections()
        print("æ•°æ®åº“è¿æ¥å·²å…³é—­")


async def process_word_file_standalone(file_path: str, doc_category: str = "Wordæ–‡æ¡£", chunk_size: int = 512):
    """
    ç‹¬ç«‹çš„Wordæ–‡æ¡£å¤„ç†å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨
    
    Args:
        file_path: Wordæ–‡æ¡£æ–‡ä»¶è·¯å¾„
        doc_category: æ–‡æ¡£ç±»åˆ«ï¼Œé»˜è®¤ä¸º"Wordæ–‡æ¡£"
        chunk_size: åˆ†å—å¤§å°ï¼Œé»˜è®¤ä¸º512
        
    Returns:
        æ’å…¥è®°å½•çš„IDåˆ—è¡¨
    """
    chunker = DocumentChunker(region_name="us-east-1")
    
    try:
        await chunker.init_db_client()
        print("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        
        inserted_ids = await process_word_document(
            chunker=chunker,
            file_path=file_path,
            doc_category=doc_category,
            chunk_size=chunk_size
        )
        
        return inserted_ids
        
    finally:
        await chunker.close_connections()
        print("æ•°æ®åº“è¿æ¥å·²å…³é—­")


def get_supported_files(directory_path: str, recursive: bool = True) -> List[Path]:
    """
    è·å–ç›®å½•ä¸­æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
    
    Args:
        directory_path: ç›®å½•è·¯å¾„
        recursive: æ˜¯å¦é€’å½’æœç´¢å­ç›®å½•
        
    Returns:
        æ”¯æŒçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    supported_extensions = {'.txt', '.docx', '.doc'}
    directory = Path(directory_path)
    
    if not directory.exists():
        raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
    
    if not directory.is_dir():
        raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")
    
    files = []
    
    if recursive:
        # é€’å½’æœç´¢æ‰€æœ‰å­ç›®å½•
        for ext in supported_extensions:
            files.extend(directory.rglob(f"*{ext}"))
    else:
        # åªæœç´¢å½“å‰ç›®å½•
        for ext in supported_extensions:
            files.extend(directory.glob(f"*{ext}"))
    
    # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶
    filtered_files = []
    for file_path in files:
        if not file_path.name.startswith('.') and not file_path.name.startswith('~'):
            filtered_files.append(file_path)
    
    return sorted(filtered_files)


async def process_directory_documents(
    directory_path: str,
    doc_category: str = "æ‰¹é‡æ–‡æ¡£",
    chunk_size: int = 512,
    recursive: bool = True,
    max_concurrent: int = 1
) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£
    
    Args:
        directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        doc_category: æ–‡æ¡£ç±»åˆ«
        chunk_size: åˆ†å—å¤§å°
        recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
        max_concurrent: æœ€å¤§å¹¶å‘å¤„ç†æ•°é‡
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡ä¿¡æ¯
    """
    print(f"å¼€å§‹æ‰¹é‡å¤„ç†ç›®å½•: {directory_path}")
    print(f"æ–‡æ¡£ç±»åˆ«: {doc_category}")
    print(f"åˆ†å—å¤§å°: {chunk_size}")
    print(f"é€’å½’å¤„ç†: {recursive}")
    print(f"æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
    print("-" * 60)
    
    # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    try:
        files = get_supported_files(directory_path, recursive)
        print(f"å‘ç° {len(files)} ä¸ªæ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
        
        if not files:
            print("æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
            return {
                'total_files': 0,
                'processed_files': 0,
                'failed_files': 0,
                'total_records': 0,
                'results': []
            }
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("\næ–‡ä»¶åˆ—è¡¨:")
        for i, file_path in enumerate(files, 1):
            print(f"  {i}. {file_path}")
        print()
        
    except Exception as e:
        print(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        raise
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    chunker = DocumentChunker(region_name="us-east-1")
    
    try:
        await chunker.init_db_client()
        print("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ\n")
        
        # å¤„ç†ç»“æœç»Ÿè®¡
        results = []
        total_records = 0
        processed_count = 0
        failed_count = 0
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_single_file(file_path: Path) -> Dict[str, Any]:
            """å¤„ç†å•ä¸ªæ–‡ä»¶"""
            async with semaphore:
                file_result = {
                    'file_path': str(file_path),
                    'file_name': file_path.name,
                    'success': False,
                    'records_count': 0,
                    'record_ids': [],
                    'error': None,
                    'processing_time': 0
                }
                
                start_time = asyncio.get_event_loop().time()
                
                try:
                    print(f"ğŸ”„ å¤„ç†æ–‡ä»¶: {file_path.name}")
                    
                    # ä½¿ç”¨chunkerçš„process_document_fileæ–¹æ³•
                    inserted_ids = await chunker.process_document_file(
                        file_path=str(file_path),
                        doc_category=doc_category,
                        chunk_size=chunk_size
                    )
                    
                    file_result['success'] = True
                    file_result['records_count'] = len(inserted_ids)
                    file_result['record_ids'] = inserted_ids
                    
                    print(f"âœ… å®Œæˆ: {file_path.name} - æ’å…¥ {len(inserted_ids)} æ¡è®°å½•")
                    
                except Exception as e:
                    file_result['error'] = str(e)
                    print(f"âŒ å¤±è´¥: {file_path.name} - {e}")
                
                finally:
                    end_time = asyncio.get_event_loop().time()
                    file_result['processing_time'] = round(end_time - start_time, 2)
                
                return file_result
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡ä»¶
        print("å¼€å§‹å¹¶å‘å¤„ç†æ–‡ä»¶...\n")
        tasks = [process_single_file(file_path) for file_path in files]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        for result in results:
            if isinstance(result, Exception):
                failed_count += 1
                print(f"âŒ å¤„ç†å¼‚å¸¸: {result}")
            elif isinstance(result, dict):
                if result['success']:
                    processed_count += 1
                    total_records += result['records_count']
                else:
                    failed_count += 1
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files)}")
        print(f"âœ… æˆåŠŸå¤„ç†: {processed_count}")
        print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
        print(f"ğŸ“ æ€»è®°å½•æ•°: {total_records}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {sum(r.get('processing_time', 0) for r in results if isinstance(r, dict)):.2f}ç§’")
        
        # æ˜¾ç¤ºå¤±è´¥çš„æ–‡ä»¶
        if failed_count > 0:
            print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶:")
            for result in results:
                if isinstance(result, dict) and not result['success']:
                    print(f"  - {result['file_name']}: {result['error']}")
        
        return {
            'total_files': len(files),
            'processed_files': processed_count,
            'failed_files': failed_count,
            'total_records': total_records,
            'results': [r for r in results if isinstance(r, dict)]
        }
        
    finally:
        await chunker.close_connections()
        print("\næ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())
    
    # æˆ–è€…ç›´æ¥å¤„ç†å•ä¸ªWordæ–‡æ¡£ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç ï¼‰
    # word_path = "your_document.docx"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
    # result = asyncio.run(process_word_file_standalone(word_path, "æŠ€æœ¯æ–‡æ¡£", 512))
    # print(f"å¤„ç†ç»“æœ: {result}")